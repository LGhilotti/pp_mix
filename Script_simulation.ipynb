{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "provincial-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import arviz as az\n",
    "import math\n",
    "# import pymc3 as pm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from google.protobuf import text_format\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from scipy.stats import skewnorm\n",
    "from scipy.stats import norm\n",
    "from scipy.interpolate import griddata\n",
    "import pp_mix.protos.py.params_pb2 as params_pb2\n",
    "\n",
    "from pp_mix.interface import ConditionalMCMC, cluster_estimate\n",
    "from pp_mix.utils import loadChains, to_numpy, to_proto\n",
    "from pp_mix.protos.py.state_pb2 import MultivariateMixtureState, EigenVector, EigenMatrix\n",
    "from pp_mix.protos.py.params_pb2 import Params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-warning",
   "metadata": {},
   "source": [
    "# Generate data\n",
    "\n",
    "assuming delta = I identical for all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "smooth-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_etas(mus, deltas_cov, cluster_alloc):\n",
    "    np.random.seed(seed=233423)\n",
    "    out = np.vstack([[mvn.rvs(mean = mus[i,:], cov = deltas_cov) for i in cluster_alloc]])\n",
    "    return out\n",
    "\n",
    "def generate_data(Lambda, etas, sigma_bar_cov):\n",
    "    np.random.seed(seed=233423)\n",
    "    means = np.matmul(Lambda,etas.T)\n",
    "    sigma_bar_cov_mat = np.diag(sigma_bar_cov)\n",
    "    out = np.vstack([[mvn.rvs(mean = means[:,i], cov = sigma_bar_cov_mat) for i in range(etas.shape[0])]])\n",
    "    return out\n",
    "\n",
    "def create_lambda(p,d):\n",
    "    if p % d != 0:\n",
    "        raise ValueError(\"Non compatible dimensions p and d: p={0}, d={1}\".format(p,d))\n",
    "    \n",
    "    h = math.floor(p/d)\n",
    "    Lambda=np.zeros((p,d))\n",
    "    for i in range(d):\n",
    "        Lambda[i*h:i*h+h,i] = np.ones(h)\n",
    "        \n",
    "    return Lambda\n",
    "\n",
    "def create_mus(d,M,dist):\n",
    "    mus = np.zeros((M,d))\n",
    "    tot_range = (M-1)*dist \n",
    "    max_mu = tot_range/2\n",
    "    for i in range(M):\n",
    "        mus[i,:] = np.repeat(max_mu-i*dist, d)\n",
    "        \n",
    "    return mus\n",
    "\n",
    "def create_cluster_alloc(n_pc,M):\n",
    "    return np.repeat(range(M),n_pc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fiscal-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "dist=5\n",
    "p_s = [50, 100]\n",
    "d_s = [2, 5]\n",
    "M_s = [4]\n",
    "n_percluster_s = [50, 200]\n",
    "\n",
    "for p in p_s:\n",
    "    sigma_bar_prec = np.repeat(2, p)\n",
    "    sigma_bar_cov = 1/sigma_bar_prec\n",
    "    for d in d_s:\n",
    "        lamb = create_lambda(p,d)\n",
    "        delta_cov = np.eye(d)\n",
    "        for M in M_s:\n",
    "            mus = create_mus(d,M,dist)\n",
    "            for n_percluster in n_percluster_s:                \n",
    "                cluster_alloc = create_cluster_alloc(n_percluster,M)\n",
    "                etas = generate_etas(mus, delta_cov, cluster_alloc)\n",
    "                data = generate_data(lamb, etas, sigma_bar_cov)\n",
    "                with open(\"data/data_script_sim/p_{0}_d_{1}_M_{2}_nperclus_{3}_data.csv\".format(p,d,M,n_percluster),\"w+\") as my_csv:\n",
    "                    csvWriter = csv.writer(my_csv, delimiter=',')\n",
    "                    csvWriter.writerows(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "charitable-vermont",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 50)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "with open(\"data/data_script_sim/p_50_d_2_M_4_nperclus_50_data.csv\", newline='') as my_csv:\n",
    "    data = pd.read_csv(my_csv, sep=',', header=None).values\n",
    "    \n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "intimate-companion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running p=50 d=2 M=4 nperclus=50\n",
      "Trick, iter # 200  /  1000\n",
      "Trick, iter # 400  /  1000\n",
      "Trick, iter # 600  /  1000\n",
      "Trick, iter # 800  /  1000\n",
      "Trick, iter # 1000  /  1000\n",
      "Burnin, iter # 200  /  15000\n",
      "Burnin, iter # 400  /  15000\n",
      "Burnin, iter # 600  /  15000\n",
      "Burnin, iter # 800  /  15000\n",
      "Burnin, iter # 1000  /  15000\n",
      "Burnin, iter # 1200  /  15000\n",
      "Burnin, iter # 1400  /  15000\n",
      "Burnin, iter # 1600  /  15000\n",
      "Burnin, iter # 1800  /  15000\n",
      "Burnin, iter # 2000  /  15000\n",
      "Burnin, iter # 2200  /  15000\n",
      "Burnin, iter # 2400  /  15000\n",
      "Burnin, iter # 2600  /  15000\n",
      "Burnin, iter # 2800  /  15000\n",
      "Burnin, iter # 3000  /  15000\n",
      "Burnin, iter # 3200  /  15000\n",
      "Burnin, iter # 3400  /  15000\n",
      "Burnin, iter # 3600  /  15000\n",
      "Burnin, iter # 3800  /  15000\n",
      "Burnin, iter # 4000  /  15000\n",
      "Burnin, iter # 4200  /  15000\n",
      "Burnin, iter # 4400  /  15000\n",
      "Burnin, iter # 4600  /  15000\n",
      "Burnin, iter # 4800  /  15000\n",
      "Burnin, iter # 5000  /  15000\n",
      "Burnin, iter # 5200  /  15000\n",
      "Burnin, iter # 5400  /  15000\n",
      "Burnin, iter # 5600  /  15000\n",
      "Burnin, iter # 5800  /  15000\n",
      "Burnin, iter # 6000  /  15000\n",
      "Burnin, iter # 6200  /  15000\n",
      "Burnin, iter # 6400  /  15000\n",
      "Burnin, iter # 6600  /  15000\n",
      "Burnin, iter # 6800  /  15000\n",
      "Burnin, iter # 7000  /  15000\n",
      "Burnin, iter # 7200  /  15000\n",
      "Burnin, iter # 7400  /  15000\n",
      "Burnin, iter # 7600  /  15000\n",
      "Burnin, iter # 7800  /  15000\n",
      "Burnin, iter # 8000  /  15000\n",
      "Burnin, iter # 8200  /  15000\n",
      "Burnin, iter # 8400  /  15000\n",
      "Burnin, iter # 8600  /  15000\n",
      "Burnin, iter # 8800  /  15000\n",
      "Burnin, iter # 9000  /  15000\n",
      "Burnin, iter # 9200  /  15000\n",
      "Burnin, iter # 9400  /  15000\n",
      "Burnin, iter # 9600  /  15000\n",
      "Burnin, iter # 9800  /  15000\n",
      "Burnin, iter # 10000  /  15000\n",
      "Burnin, iter # 10200  /  15000\n",
      "Burnin, iter # 10400  /  15000\n",
      "Burnin, iter # 10600  /  15000\n",
      "Burnin, iter # 10800  /  15000\n",
      "Burnin, iter # 11000  /  15000\n",
      "Burnin, iter # 11200  /  15000\n",
      "Burnin, iter # 11400  /  15000\n",
      "Burnin, iter # 11600  /  15000\n",
      "Burnin, iter # 11800  /  15000\n",
      "Burnin, iter # 12000  /  15000\n",
      "Burnin, iter # 12200  /  15000\n",
      "Burnin, iter # 12400  /  15000\n",
      "Burnin, iter # 12600  /  15000\n",
      "Burnin, iter # 12800  /  15000\n",
      "Burnin, iter # 13000  /  15000\n",
      "Burnin, iter # 13200  /  15000\n",
      "Burnin, iter # 13400  /  15000\n",
      "Burnin, iter # 13600  /  15000\n",
      "Burnin, iter # 13800  /  15000\n",
      "Burnin, iter # 14000  /  15000\n",
      "Burnin, iter # 14200  /  15000\n",
      "Burnin, iter # 14400  /  15000\n",
      "Burnin, iter # 14600  /  15000\n",
      "Burnin, iter # 14800  /  15000\n",
      "Burnin, iter # 15000  /  15000\n",
      "Running, iter # 200  /  15000\n",
      "Running, iter # 400  /  15000\n",
      "Running, iter # 600  /  15000\n",
      "Running, iter # 800  /  15000\n",
      "Running, iter # 1000  /  15000\n",
      "Running, iter # 1200  /  15000\n",
      "Running, iter # 1400  /  15000\n",
      "Running, iter # 1600  /  15000\n",
      "Running, iter # 1800  /  15000\n",
      "Running, iter # 2000  /  15000\n",
      "Running, iter # 2200  /  15000\n",
      "Running, iter # 2400  /  15000\n",
      "Running, iter # 2600  /  15000\n",
      "Running, iter # 2800  /  15000\n",
      "Running, iter # 3000  /  15000\n",
      "Running, iter # 3200  /  15000\n",
      "Running, iter # 3400  /  15000\n",
      "Running, iter # 3600  /  15000\n",
      "Running, iter # 3800  /  15000\n",
      "Running, iter # 4000  /  15000\n",
      "Running, iter # 4200  /  15000\n",
      "Running, iter # 4400  /  15000\n",
      "Running, iter # 4600  /  15000\n",
      "Running, iter # 4800  /  15000\n",
      "Running, iter # 5000  /  15000\n",
      "Running, iter # 5200  /  15000\n",
      "Running, iter # 5400  /  15000\n",
      "Running, iter # 5600  /  15000\n",
      "Running, iter # 5800  /  15000\n",
      "Running, iter # 6000  /  15000\n",
      "Running, iter # 6200  /  15000\n",
      "Running, iter # 6400  /  15000\n",
      "Running, iter # 6600  /  15000\n",
      "Running, iter # 6800  /  15000\n",
      "Running, iter # 7000  /  15000\n",
      "Running, iter # 7200  /  15000\n",
      "Running, iter # 7400  /  15000\n",
      "Running, iter # 7600  /  15000\n",
      "Running, iter # 7800  /  15000\n",
      "Running, iter # 8000  /  15000\n",
      "Running, iter # 8200  /  15000\n",
      "Running, iter # 8400  /  15000\n",
      "Running, iter # 8600  /  15000\n",
      "Running, iter # 8800  /  15000\n",
      "Running, iter # 9000  /  15000\n",
      "Running, iter # 9200  /  15000\n",
      "Running, iter # 9400  /  15000\n",
      "Running, iter # 9600  /  15000\n",
      "Running, iter # 9800  /  15000\n",
      "Running, iter # 10000  /  15000\n",
      "Running, iter # 10200  /  15000\n",
      "Running, iter # 10400  /  15000\n",
      "Running, iter # 10600  /  15000\n",
      "Running, iter # 10800  /  15000\n",
      "Running, iter # 11000  /  15000\n",
      "Running, iter # 11200  /  15000\n",
      "Running, iter # 11400  /  15000\n",
      "Running, iter # 11600  /  15000\n",
      "Running, iter # 11800  /  15000\n",
      "Running, iter # 12000  /  15000\n",
      "Running, iter # 12200  /  15000\n",
      "Running, iter # 12400  /  15000\n",
      "Running, iter # 12600  /  15000\n",
      "Running, iter # 12800  /  15000\n",
      "Running, iter # 13000  /  15000\n",
      "Running, iter # 13200  /  15000\n",
      "Running, iter # 13400  /  15000\n",
      "Running, iter # 13600  /  15000\n",
      "Running, iter # 13800  /  15000\n",
      "Running, iter # 14000  /  15000\n",
      "Running, iter # 14200  /  15000\n",
      "Running, iter # 14400  /  15000\n",
      "Running, iter # 14600  /  15000\n",
      "Running, iter # 14800  /  15000\n",
      "Running, iter # 15000  /  15000\n",
      "Allocated Means acceptance rate  0.77746711979882221488225013672490604221820831298828125\n",
      "Lambda acceptance rate  0.07509677419354839067455742451784317381680011749267578125\n",
      "Running p=50 d=2 M=4 nperclus=200\n",
      "Trick, iter # 200  /  1000\n",
      "Trick, iter # 400  /  1000\n",
      "Trick, iter # 600  /  1000\n",
      "Trick, iter # 800  /  1000\n",
      "Trick, iter # 1000  /  1000\n",
      "Burnin, iter # 200  /  15000\n",
      "Burnin, iter # 400  /  15000\n",
      "Burnin, iter # 600  /  15000\n",
      "Burnin, iter # 800  /  15000\n",
      "Burnin, iter # 1000  /  15000\n",
      "Burnin, iter # 1200  /  15000\n",
      "Burnin, iter # 1400  /  15000\n",
      "Burnin, iter # 1600  /  15000\n",
      "Burnin, iter # 1800  /  15000\n",
      "Burnin, iter # 2000  /  15000\n",
      "Burnin, iter # 2200  /  15000\n",
      "Burnin, iter # 2400  /  15000\n",
      "Burnin, iter # 2600  /  15000\n",
      "Burnin, iter # 2800  /  15000\n",
      "Burnin, iter # 3000  /  15000\n",
      "Burnin, iter # 3200  /  15000\n",
      "Burnin, iter # 3400  /  15000\n",
      "Burnin, iter # 3600  /  15000\n",
      "Burnin, iter # 3800  /  15000\n",
      "Burnin, iter # 4000  /  15000\n",
      "Burnin, iter # 4200  /  15000\n",
      "Burnin, iter # 4400  /  15000\n",
      "Burnin, iter # 4600  /  15000\n",
      "Burnin, iter # 4800  /  15000\n",
      "Burnin, iter # 5000  /  15000\n",
      "Burnin, iter # 5200  /  15000\n",
      "Burnin, iter # 5400  /  15000\n",
      "Burnin, iter # 5600  /  15000\n",
      "Burnin, iter # 5800  /  15000\n",
      "Burnin, iter # 6000  /  15000\n",
      "Burnin, iter # 6200  /  15000\n",
      "Burnin, iter # 6400  /  15000\n",
      "Burnin, iter # 6600  /  15000\n",
      "Burnin, iter # 6800  /  15000\n",
      "Burnin, iter # 7000  /  15000\n",
      "Burnin, iter # 7200  /  15000\n",
      "Burnin, iter # 7400  /  15000\n",
      "Burnin, iter # 7600  /  15000\n",
      "Burnin, iter # 7800  /  15000\n",
      "Burnin, iter # 8000  /  15000\n",
      "Burnin, iter # 8200  /  15000\n",
      "Burnin, iter # 8400  /  15000\n",
      "Burnin, iter # 8600  /  15000\n",
      "Burnin, iter # 8800  /  15000\n",
      "Burnin, iter # 9000  /  15000\n",
      "Burnin, iter # 9200  /  15000\n",
      "Burnin, iter # 9400  /  15000\n",
      "Burnin, iter # 9600  /  15000\n",
      "Burnin, iter # 9800  /  15000\n",
      "Burnin, iter # 10000  /  15000\n",
      "Burnin, iter # 10200  /  15000\n",
      "Burnin, iter # 10400  /  15000\n",
      "Burnin, iter # 10600  /  15000\n",
      "Burnin, iter # 10800  /  15000\n",
      "Burnin, iter # 11000  /  15000\n",
      "Burnin, iter # 11200  /  15000\n",
      "Burnin, iter # 11400  /  15000\n",
      "Burnin, iter # 11600  /  15000\n",
      "Burnin, iter # 11800  /  15000\n",
      "Burnin, iter # 12000  /  15000\n",
      "Burnin, iter # 12200  /  15000\n",
      "Burnin, iter # 12400  /  15000\n",
      "Burnin, iter # 12600  /  15000\n",
      "Burnin, iter # 12800  /  15000\n",
      "Burnin, iter # 13000  /  15000\n",
      "Burnin, iter # 13200  /  15000\n",
      "Burnin, iter # 13400  /  15000\n",
      "Burnin, iter # 13600  /  15000\n",
      "Burnin, iter # 13800  /  15000\n",
      "Burnin, iter # 14000  /  15000\n",
      "Burnin, iter # 14200  /  15000\n",
      "Burnin, iter # 14400  /  15000\n",
      "Burnin, iter # 14600  /  15000\n",
      "Burnin, iter # 14800  /  15000\n",
      "Burnin, iter # 15000  /  15000\n",
      "Running, iter # 200  /  15000\n",
      "Running, iter # 400  /  15000\n",
      "Running, iter # 600  /  15000\n",
      "Running, iter # 800  /  15000\n",
      "Running, iter # 1000  /  15000\n",
      "Running, iter # 1200  /  15000\n",
      "Running, iter # 1400  /  15000\n",
      "Running, iter # 1600  /  15000\n",
      "Running, iter # 1800  /  15000\n",
      "Running, iter # 2000  /  15000\n",
      "Running, iter # 2200  /  15000\n",
      "Running, iter # 2400  /  15000\n",
      "Running, iter # 2600  /  15000\n",
      "Running, iter # 2800  /  15000\n",
      "Running, iter # 3000  /  15000\n",
      "Running, iter # 3200  /  15000\n",
      "Running, iter # 3400  /  15000\n",
      "Running, iter # 3600  /  15000\n",
      "Running, iter # 3800  /  15000\n",
      "Running, iter # 4000  /  15000\n",
      "Running, iter # 4200  /  15000\n",
      "Running, iter # 4400  /  15000\n",
      "Running, iter # 4600  /  15000\n",
      "Running, iter # 4800  /  15000\n",
      "Running, iter # 5000  /  15000\n",
      "Running, iter # 5200  /  15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running, iter # 5400  /  15000\n",
      "Running, iter # 5600  /  15000\n",
      "Running, iter # 5800  /  15000\n",
      "Running, iter # 6000  /  15000\n",
      "Running, iter # 6200  /  15000\n",
      "Running, iter # 6400  /  15000\n",
      "Running, iter # 6600  /  15000\n",
      "Running, iter # 6800  /  15000\n",
      "Running, iter # 7000  /  15000\n",
      "Running, iter # 7200  /  15000\n",
      "Running, iter # 7400  /  15000\n",
      "Running, iter # 7600  /  15000\n",
      "Running, iter # 7800  /  15000\n",
      "Running, iter # 8000  /  15000\n",
      "Running, iter # 8200  /  15000\n",
      "Running, iter # 8400  /  15000\n",
      "Running, iter # 8600  /  15000\n",
      "Running, iter # 8800  /  15000\n",
      "Running, iter # 9000  /  15000\n",
      "Running, iter # 9200  /  15000\n",
      "Running, iter # 9400  /  15000\n",
      "Running, iter # 9600  /  15000\n",
      "Running, iter # 9800  /  15000\n",
      "Running, iter # 10000  /  15000\n",
      "Running, iter # 10200  /  15000\n",
      "Running, iter # 10400  /  15000\n",
      "Running, iter # 10600  /  15000\n",
      "Running, iter # 10800  /  15000\n",
      "Running, iter # 11000  /  15000\n",
      "Running, iter # 11200  /  15000\n",
      "Running, iter # 11400  /  15000\n",
      "Running, iter # 11600  /  15000\n",
      "Running, iter # 11800  /  15000\n",
      "Running, iter # 12000  /  15000\n",
      "Running, iter # 12200  /  15000\n",
      "Running, iter # 12400  /  15000\n",
      "Running, iter # 12600  /  15000\n",
      "Running, iter # 12800  /  15000\n",
      "Running, iter # 13000  /  15000\n",
      "Running, iter # 13200  /  15000\n",
      "Running, iter # 13400  /  15000\n",
      "Running, iter # 13600  /  15000\n",
      "Running, iter # 13800  /  15000\n",
      "Running, iter # 14000  /  15000\n",
      "Running, iter # 14200  /  15000\n",
      "Running, iter # 14400  /  15000\n",
      "Running, iter # 14600  /  15000\n",
      "Running, iter # 14800  /  15000\n",
      "Running, iter # 15000  /  15000\n",
      "Allocated Means acceptance rate  0.17944323758950841973813794538727961480617523193359375\n",
      "Lambda acceptance rate  0.1677419354838709797395068790137884207069873809814453125\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "p_s = [50] #p_s = [50, 100]\n",
    "d_s = [2] #d_s = [2, 5]\n",
    "M_s = [4]\n",
    "n_percluster_s = [50, 200]\n",
    "\n",
    "ntrick = 1000\n",
    "nburn = 15000\n",
    "niter = 15000\n",
    "thin = 10\n",
    "\n",
    "loops = [x for x in itertools.product(p_s,d_s,M_s,n_percluster_s)]\n",
    "\n",
    "for p, d, M, n_percluster in loops:\n",
    "    print(\"Running p={0} d={1} M={2} nperclus={3}\".format(p,d,M,n_percluster))\n",
    "    with open(\"data/data_script_sim/p_{0}_d_{1}_M_{2}_nperclus_{3}_data.csv\".format(p,d,M,n_percluster), newline='') as my_csv:\n",
    "        data = pd.read_csv(my_csv, sep=',', header=None).values\n",
    "    ranges = np.array([[-50,50],]*d).transpose()\n",
    "\n",
    "    sampler = ConditionalMCMC(params_file = \"data/data_script_sim/params/sampler_params_p_{0}_d_{1}_M_{2}_n_{3}.asciipb\".format(p,d,M,n_percluster))\n",
    "    sampler.run(ntrick, nburn, niter, thin, data, ranges, 200)\n",
    "    sampler.serialize_chains(\"data/data_script_sim/chains/chains_p_{0}_d_{1}_M_{2}_nperclus_{3}.recordio\".format(p,d,M,n_percluster))\n",
    "    acc_rates = np.array([sampler.means_ar, sampler.lambda_ar])\n",
    "    np.savetxt(\"data/data_script_sim/chains/acc_rate_p_{0}_d_{1}_M_{2}_nperclus_{3}.csv\".format(p,d,M,n_percluster), acc_rates, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "comparative-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain0 = loadChains(\"data/data_script_sim/chains/chains_p_50_d_2_M_4_nperclus_50.recordio\", MultivariateMixtureState)\n",
    "#chain1 = loadChains(\"data/data_script_sim/chains/chains_p_50_d_5_M_4_nperclus_50.recordio\", MultivariateMixtureState)\n",
    "chain2 = loadChains(\"data/data_script_sim/chains/chains_p_50_d_2_M_4_nperclus_200.recordio\", MultivariateMixtureState)\n",
    "#chain3 = loadChains(\"data/data_script_sim/chains/chains_p_50_d_5_M_4_nperclus_200.recordio\", MultivariateMixtureState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "equipped-religion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "0.4513333333333333\n",
      "4.0\n",
      "0.46\n"
     ]
    }
   ],
   "source": [
    "nc_chain0 = np.array([x.ma for x in chain0])\n",
    "nonall_0 = np.array([x.mna for x in chain0])\n",
    "#nc_chain1 = np.array([x.ma for x in chain1])\n",
    "nc_chain2 = np.array([x.ma for x in chain2])\n",
    "nonall_2 = np.array([x.mna for x in chain2])\n",
    "\n",
    "#nc_chain3 = np.array([x.ma for x in chain3])\n",
    "\n",
    "print(nc_chain0.mean())\n",
    "print(nonall_0.mean())\n",
    "\n",
    "#print(nc_chain1.mean())\n",
    "print(nc_chain2.mean())\n",
    "print(nonall_2.mean())\n",
    "\n",
    "#print(nc_chain3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "color-selling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2[-1].clus_alloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-pride",
   "metadata": {},
   "source": [
    "# Check num_means in trick phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "p_s = [50]\n",
    "d_s = [5]\n",
    "M_s = [4]\n",
    "n_percluster_s = [200]\n",
    "\n",
    "ntrick = 0\n",
    "nburn = 0\n",
    "niter = 0\n",
    "thin = 1\n",
    "\n",
    "loops = [x for x in itertools.product(p_s,d_s,M_s,n_percluster_s)]\n",
    "\n",
    "for p, d, M, n_percluster in loops:\n",
    "    print(\"Running p={0} d={1} M={2} nperclus={3}\".format(p,d,M,n_percluster))\n",
    "    with open(\"data/data_script_sim/p_{0}_d_{1}_M_{2}_nperclus_{3}_data.csv\".format(p,d,M,n_percluster), newline='') as my_csv:\n",
    "        data = pd.read_csv(my_csv, sep=',', header=None).values\n",
    "    ranges = np.array([[-50,50],]*d).transpose()\n",
    "\n",
    "    sampler = ConditionalMCMC(params_file = \"data/data_script_sim/params/sampler_params_p_{0}_d_{1}_M_{2}_n_{3}.asciipb\".format(p,d,M,n_percluster))\n",
    "    sampler.run(ntrick, nburn, niter, thin, data, ranges, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, d, M, n_percluster in itertools.product(p_s,d_s,M_s,n_percluster_s):\n",
    "    print(\"Running p={0} d={1} M={2} nperclus={3}\".format(p,d,M,n_percluster))   \n",
    "    data = np.loadtxt(\"data/data_script_sim/p_{0}_d_{1}_M_{2}_nperclus_{3}_data.csv\".format(p,d,M,n_percluster))\n",
    "    ranges = np.array([[-50,50],]*d).transpose()\n",
    "    sampler = ConditionalMCMC(params_file = \"data/data_script_sim/params/sampler_params_p_{0}_d_{1}_M_{2}_n_{3}.asciipb\".format(p,d,M,n_percluster))\n",
    "    sampler.run(ntrick, nburn, niter, thin, data, ranges, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "desirable-comment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    399\n",
       "5      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ser = pd.Series(nc_chain2)\n",
    "ser.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import pandas as pd\n",
    "\n",
    "p_s = [50] #p_s = [50, 100]\n",
    "d_s = [2, 5]\n",
    "M_s = [4]\n",
    "n_percluster_s = [50, 200]\n",
    "list_performance = list()\n",
    "\n",
    "\n",
    "for p, d, M, n_percluster in itertools.product(p_s,d_s,M_s,n_percluster_s):\n",
    "    chain = loadChains(\"data/data_script_sim/chains/chains_p_{0}_d_{1}_M_{2}_nperclus_{3}.recordio\".format(p,d,M,n_percluster), MultivariateMixtureState)\n",
    "    acc_rates = np.loadtxt(\"data/data_script_sim/chains/acc_rate_p_{0}_d_{1}_M_{2}_nperclus_{3}.csv\".format(p,d,M,n_percluster), delimiter=',')\n",
    "\n",
    "    n_cluster_chain = np.array([x.ma for x in chain])\n",
    "    post_mode_nclus = stats.mode(n_cluster_chain)[0][0] # store in dataframe\n",
    "    post_avg_nclus = n_cluster_chain.mean() # store in dataframe\n",
    "    \n",
    "    clus_alloc_chain = [x.clus_alloc for x in chain]\n",
    "    best_clus = cluster_estimate(np.array(clus_alloc_chain))\n",
    "    true_clus = np.repeat(range(M),n_percluster)\n",
    "    ari_best_clus = adjusted_rand_score(true_clus, best_clus) # store in dataframe\n",
    "    \n",
    "    aris_chain = np.array([adjusted_rand_score(true_clus, x) for x in clus_alloc_chain])\n",
    "    mean_aris, sigma_aris = np.mean(aris_chain), np.std(aris_chain) # store mean_aris in dataframe\n",
    "    CI_aris = stats.norm.interval(0.95, loc=mean_aris, scale=sigma_aris/sqrt(len(aris_chain))) # store in dataframe\n",
    "    \n",
    "    list_performance.append([p,d,M,n_percluster,acc_rates[0],acc_rates[1],post_mode_nclus,\n",
    "                        post_avg_nclus,ari_best_clus,mean_aris,CI_aris])\n",
    "    \n",
    "\n",
    "    \n",
    "df_performance = pd.DataFrame(list_performance, columns=('dim_p', 'dim_d', 'dim_M', 'dim_n_pc', 'means_ar','lambda_ar',\n",
    "                                      'mode_nclus', 'avg_nclus', 'ari_best_clus', 'mean_ari', 'CI_ari'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "common-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_s=[4]\n",
    "d_s=[2]\n",
    "n_percluster_s = [4]\n",
    "M_s=[2]\n",
    "dist=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "utility-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in p_s:\n",
    "    sigma_bar_prec = np.repeat(2, p)\n",
    "    sigma_bar_cov = 1/sigma_bar_prec\n",
    "    for d in d_s:\n",
    "        lamb = create_lambda(p,d)\n",
    "        delta_cov = np.eye(d)\n",
    "        for M in M_s:\n",
    "            mus = create_mus(d,M,dist)\n",
    "            for n_percluster in n_percluster_s:                \n",
    "                cluster_alloc = create_cluster_alloc(n_percluster,M)\n",
    "                etas = generate_etas(mus, delta_cov, cluster_alloc)\n",
    "                data = generate_data(lamb, etas, sigma_bar_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unusual-limit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trick, iter # 2  /  10\n",
      "Trick, iter # 4  /  10\n",
      "Trick, iter # 6  /  10\n",
      "Trick, iter # 8  /  10\n",
      "Trick, iter # 10  /  10\n",
      "Burnin, iter # 2  /  10\n",
      "Burnin, iter # 4  /  10\n",
      "Burnin, iter # 6  /  10\n",
      "Burnin, iter # 8  /  10\n",
      "Burnin, iter # 10  /  10\n",
      "Running, iter # 2  /  10\n",
      "Running, iter # 4  /  10\n",
      "Running, iter # 6  /  10\n",
      "Running, iter # 8  /  10\n",
      "Running, iter # 10  /  10\n",
      "Allocated Means acceptance rate  0.94897959183673474825582161429338157176971435546875\n",
      "Lambda acceptance rate  0.96666666666666667406815349750104360282421112060546875\n"
     ]
    }
   ],
   "source": [
    "params_file_1 = \"pp_mix/resources/sampler_params_1.asciipb\"\n",
    "ranges = np.array([[-50,50],]*2).transpose()\n",
    "\n",
    "sampler = ConditionalMCMC(params_file = params_file_1)\n",
    "sampler.run(10, 10, 10, 1, data, ranges, 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "floating-silicon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.lambda_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "running-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in p_s:\n",
    "    sigma_bar_prec = np.repeat(2, p)\n",
    "    sigma_bar_cov = 1/sigma_bar_prec\n",
    "    for d in d_s:\n",
    "        lamb = create_lambda(p,d)\n",
    "        delta_cov = np.eye(d)\n",
    "        for M in M_s:\n",
    "            mus = create_mus(d,M,dist)\n",
    "            for n_percluster in n_percluster_s:                \n",
    "                cluster_alloc = create_cluster_alloc(n_percluster,M)\n",
    "                etas = generate_etas(mus, delta_cov, cluster_alloc)\n",
    "                data = generate_data(lamb, etas, sigma_bar_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fatal-delaware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.28887113,  2.1098959 ,  3.10500424,  4.78220331],\n",
       "       [ 3.38574598,  2.31299995,  4.58518628,  4.50645221],\n",
       "       [ 2.98161869,  3.08663616,  2.52882726,  2.23625688],\n",
       "       [ 0.91001833,  1.70168939,  1.80135309,  0.8923229 ],\n",
       "       [-2.66042942, -1.44525927, -2.04019421, -2.84734837],\n",
       "       [-2.50785665, -2.04884576, -1.46844031, -3.04706031],\n",
       "       [-3.45004754, -4.54361109, -2.93390446, -2.39452052],\n",
       "       [-2.04601252, -2.60295521, -4.22756683, -4.3434902 ]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "metric-rates",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o= create_cluster_alloc(14,3)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fuzzy-century",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(11/3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
